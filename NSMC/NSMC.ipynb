{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conservative-checklist",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "java.nio.file.InvalidPathException: Illegal char <*> at index 72: c:\\users\\momozzing\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\konlpy\\java\\*",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-efad08fa1137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 형태소 분석을 위한 객체 생성.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mkkma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKkma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mkomoran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mhannanum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\momozzing\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misJVMStarted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mkkmaJavaPackage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kr.lucypark.kkma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\momozzing\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\konlpy\\jvm.py\u001b[0m in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                 \u001b[1;34m'-Dfile.encoding=UTF8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                 \u001b[1;34m'-ea'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-Xmx{}m'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                 convertStrings=True)\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please specify the JVM path.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\momozzing\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\jpype\\_core.py\u001b[0m in \u001b[0;36mstartJVM\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         _jpype.startup(jvmpath, tuple(args),\n\u001b[1;32m--> 222\u001b[1;33m                        ignoreUnrecognized, convertStrings, interrupt)\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[0minitializeResources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: java.nio.file.InvalidPathException: Illegal char <*> at index 72: c:\\users\\momozzing\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\konlpy\\java\\*"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re \n",
    "# 형태소 분석을 위한 객체 생성.\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "mecab = Mecab()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"안녕 나는 윤호라고해\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-challenge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mecab.pos(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-avenue",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-reason",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-accent",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())\n",
    "train_data = train_data.fillna(str(0))\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.isnull().sum(0))\n",
    "test_data = test_data.fillna(str(0))\n",
    "print(test_data.isnull().sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "stopwords=['이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', \n",
    "'년', '가', '한', '지', '대하', '오', '일', '그렇', '위하', '때문', '그것', '두', '말하', '알', '그러나', '받', '못하', '일', '그런', '또',\n",
    "'문제', '더', '사회', '많', '그리고', '크', '따르', '중', '나오', '가지', '씨', '시키', '만들', '지금', '생각하', '그러', '속', '하나', '집',\n",
    "'살', '모르', '적', '월', '데', '자신', '안', '어떤', '내', '경우', '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', \n",
    "'다른', '어떻', '여자', '개', '전', '들', '사실', '이렇', '점', '싶', '정도', '좀', '원', '통하', '놓']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:150000] \n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = mecab.morphs(sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[:50000] \n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = mecab.morphs(sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-oklahoma",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-indianapolis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token=mecab.morphs(token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=mecab.morphs(token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아옵니다.\n",
    "            bow[index]=bow[index]+1\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-occasions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-offering",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-nickel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-french",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(train_data['label'])\n",
    "Y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "Y_train = np.delete(Y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GauNB = GaussianNB()\n",
    "GauNB.fit(X_train, Y_train)\n",
    "GauNB_pred = GauNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도:\", accuracy_score(Y_test, GauNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BerNB = BernoulliNB()\n",
    "BerNB.fit(X_train, Y_train)\n",
    "BerNB_pred = BerNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-charger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"정확도:\", accuracy_score(Y_test, BerNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=20, random_state=24)\n",
    "\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도:\", accuracy_score(Y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(rfc.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('와 개쩐다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 재밌다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-operation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "canadian-wealth",
   "metadata": {},
   "source": [
    "# 일단 이거는 binary classifier\n",
    "\n",
    "이거를 풀기위해서 일단은 전처리 파트를 공부를 해야해요 \n",
    "\n",
    "각자의 데이터셋을 만들어오기! <--- 중요!\n",
    "\n",
    "               doucment\n",
    "          3D/만 /아니었어도/ 별 /다섯 /개 줬을/텐데/.. 왜 /3D/로/ 나와서 /제 /심기/를 불편/하게/ 하/죠/?/?/  이런식으로 분류되게  만들어오기\n",
    "          \n",
    "          \n",
    "전처리 끝내놓으면 이제 모델에 넣고 하기 편해가지고 전처리 파트 공부해서 전처리 하고 다시 모이기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-transmission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
