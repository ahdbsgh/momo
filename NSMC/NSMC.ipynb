{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continental-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re \n",
    "# 형태소 분석을 위한 객체 생성.\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "mecab = Mecab()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indonesian-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"안녕 나는 윤호라고해\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-tutorial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('안녕', 'IC'),\n",
       " ('나', 'NP'),\n",
       " ('는', 'JX'),\n",
       " ('윤호', 'NNP'),\n",
       " ('라고', 'VCP+EC'),\n",
       " ('해', 'VV+EC')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mecab.pos(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vanilla-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "banner-swedish",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6483659</td>\n      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>149995</th>\n      <td>6222902</td>\n      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>149996</th>\n      <td>8549745</td>\n      <td>평점이 너무 낮아서...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149997</th>\n      <td>9311800</td>\n      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>149998</th>\n      <td>2376369</td>\n      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149999</th>\n      <td>9619869</td>\n      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>150000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-litigation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())\n",
    "train_data = train_data.fillna(str(0))\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.isnull().sum(0))\n",
    "test_data = test_data.fillna(str(0))\n",
    "print(test_data.isnull().sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "stopwords=['이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', \n",
    "'년', '가', '한', '지', '대하', '오', '일', '그렇', '위하', '때문', '그것', '두', '말하', '알', '그러나', '받', '못하', '일', '그런', '또',\n",
    "'문제', '더', '사회', '많', '그리고', '크', '따르', '중', '나오', '가지', '씨', '시키', '만들', '지금', '생각하', '그러', '속', '하나', '집',\n",
    "'살', '모르', '적', '월', '데', '자신', '안', '어떤', '내', '경우', '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', \n",
    "'다른', '어떻', '여자', '개', '전', '들', '사실', '이렇', '점', '싶', '정도', '좀', '원', '통하', '놓']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:150000] \n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = mecab.morphs(sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[:50000] \n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = mecab.morphs(sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-survival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-dinner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token=mecab.morphs(token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=mecab.morphs(token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아옵니다.\n",
    "            bow[index]=bow[index]+1\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-resident",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-farmer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-benefit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(train_data['label'])\n",
    "Y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "Y_train = np.delete(Y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GauNB = GaussianNB()\n",
    "GauNB.fit(X_train, Y_train)\n",
    "GauNB_pred = GauNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도:\", accuracy_score(Y_test, GauNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BerNB = BernoulliNB()\n",
    "BerNB.fit(X_train, Y_train)\n",
    "BerNB_pred = BerNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-sailing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"정확도:\", accuracy_score(Y_test, BerNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=20, random_state=24)\n",
    "\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도:\", accuracy_score(Y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(rfc.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('와 개쩐다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 재밌다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-disease",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "considerable-providence",
   "metadata": {},
   "source": [
    "# 일단 이거는 binary classifier\n",
    "\n",
    "이거를 풀기위해서 일단은 전처리 파트를 공부를 해야해요 \n",
    "\n",
    "각자의 데이터셋을 만들어오기! <--- 중요!\n",
    "\n",
    "               doucment\n",
    "          3D/만 /아니었어도/ 별 /다섯 /개 줬을/텐데/.. 왜 /3D/로/ 나와서 /제 /심기/를 불편/하게/ 하/죠/?/?/  이런식으로 분류되게  만들어오기\n",
    "          \n",
    "          \n",
    "전처리 끝내놓으면 이제 모델에 넣고 하기 편해가지고 전처리 파트 공부해서 전처리 하고 다시 모이기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-beast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-output",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0f8af005c6536e801c34ed1a329b6361f501640f22aff193c57cfc4ad0e1dbb64",
   "display_name": "Python 3.7.10 64-bit ('momo': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}