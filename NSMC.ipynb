{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 형태소 분석을 위한 객체 생성.\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "mecab = Mecab()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"안녕 나는 윤호라고해\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-challenge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mecab.pos(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(r'C:\\Users\\Gachon\\WorkSpace\\momo/ratings_train.txt')\n",
    "test_data = pd.read_table(r'C:\\Users\\Gachon\\WorkSpace\\momo/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-avenue",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-accent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())\n",
    "train_data = train_data.fillna(str(0))\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_data['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '너무재밓었다그래서보는것을추천한다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())\n",
    "train_data = train_data.fillna(str(0))\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.fillna(str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.isnull().sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.fillna(str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.isnull().sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-sheffield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 임의의 10000개의 리뷰를 sample data로 사용한다.\n",
    "sample_data = train_data[:10000] \n",
    "\n",
    "# 한글과 공백을 제외하고 불필요한 문자는 모두 제거한다. - 정규표현식 사용\n",
    "sample_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "# 불용어를 제거해준다. - 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등은 검색 색인 단어로 의미가 없는 단어\n",
    "stopwords=['뭐','으면','을','의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "mecab = Mecab()\n",
    "res = []\n",
    "for sentence in sample_data['document']:\n",
    "#     tmp = []\n",
    "    tmp = mecab.morphs(sentence)\n",
    "  \n",
    "    tokenized = []\n",
    "    for token in tmp:\n",
    "        if not token in stopwords:\n",
    "            tokenized.append(token)\n",
    "\n",
    "    res.append(tokenized)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:150000] \n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = mecab.morphs(sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[:50000] \n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = mecab.morphs(sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-arena",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = sum(res, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-lithuania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in answer:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아옵니다.\n",
    "            bow[index]=bow[index]+1\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-french",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-eagle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = sum(res, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-determination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-hanging",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizedd = pd.DataFrame(data = res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-spell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-chuck",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-virginia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizedd = tokenizedd.fillna(str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-dialogue",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = tokenizedd\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectorizer.fit(documents)\n",
    "bow_vector = bow_vectorizer.transform(test_text)\n",
    "print(bow_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-monthly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-league",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-marshall",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-richmond",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-wealth",
   "metadata": {},
   "source": [
    "# 일단 이거는 binary classifier\n",
    "\n",
    "이거를 풀기위해서 일단은 전처리 파트를 공부를 해야해요 \n",
    "\n",
    "각자의 데이터셋을 만들어오기! <--- 중요!\n",
    "\n",
    "               doucment\n",
    "          3D/만 /아니었어도/ 별 /다섯 /개 줬을/텐데/.. 왜 /3D/로/ 나와서 /제 /심기/를 불편/하게/ 하/죠/?/?/  이런식으로 분류되게  만들어오기\n",
    "          \n",
    "          \n",
    "전처리 끝내놓으면 이제 모델에 넣고 하기 편해가지고 전처리 파트 공부해서 전처리 하고 다시 모이기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-identity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-transmission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
